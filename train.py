from __future__ import print_function
# from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils
from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

from os import path
import sys
import numpy as np
import resnet
from config import DATA_DIR, TRAINX, TRAINY, TESTX, shape, nb_classes


num_layers = 101
f_model = path.join(DATA_DIR, 'model')
model_name = 'resnet_classweight%d' % num_layers
csv_file = path.join('./log', '%s.csv' % model_name)

if not path.exists(f_model):
    raise ValueError('%s not exists' % f_model)

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)
early_stopper = EarlyStopping(min_delta=0.001, patience=20)
csv_logger = CSVLogger(csv_file)
checkpoint = ModelCheckpoint(filepath=path.join(f_model,
                                                '%s_model{epoch:02d}-loss{loss:.2f}-acc{acc:.2f}-vloss{val_loss:.2f}-vacc{val_acc:.2f}.hdf5'
                                                % model_name), monitor='val_loss', verbose=1, save_best_only=True, mode='auto')

batch_size = 128
nb_epoch = 400
data_augmentation = True

# input image dimensions
img_rows, img_cols = shape
# RGB.
img_channels = 3

train_X = np.load(TRAINX)
train_y_origin = np.load(TRAINY)
test_X = np.load(TESTX)

# Convert class vectors to binary class matrices.
train_y = np_utils.to_categorical(train_y_origin, nb_classes)

train_X = train_X.astype('float32')
test_X = test_X.astype('float32')

# subtract mean and normalize
# mean_image = np.mean(train_X, axis=0)
# train_X -= mean_image
# test_X -= mean_image
# train_X /= 128.
# test_X /= 128.

print(train_y_origin, nb_classes)
class_weight = compute_class_weight('balanced', np.unique(train_y_origin), train_y_origin)

print(train_X.shape, train_y.shape, class_weight)

# split for validation
train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.2, random_state=1234)

model = resnet.ResnetBuilder.build_resnet_by(num_layers, (img_channels, img_rows, img_cols), nb_classes)
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# load checkpoint
load_file = sys.argv[1] if len(sys.argv) > 1 else ''
if path.exists(load_file):
    print('load checkpoint', load_file)
    model.load_weights(load_file)

if not data_augmentation:
    print('Not using data augmentation.')
    model.fit(train_X, train_y,
              batch_size=batch_size,
              nb_epoch=nb_epoch,
              validation_data=(valid_X, valid_y),
              shuffle=True,
              class_weight=class_weight,
              callbacks=[lr_reducer, early_stopper, csv_logger, checkpoint])
else:
    print('Using real-time data augmentation.')
    # This will do preprocessing and realtime data augmentation:
    datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)
        zoom_range=0.4,
        horizontal_flip=True,  # randomly flip images
        vertical_flip=True)  # randomly flip images

    # Compute quantities required for featurewise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
    datagen.fit(train_X)

    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(datagen.flow(train_X, train_y, batch_size=batch_size),
                        steps_per_epoch=train_X.shape[0] // batch_size,
                        epochs=nb_epoch, verbose=1, max_q_size=batch_size * 3,
                        validation_data=(valid_X, valid_y),
                        class_weight=class_weight,
                        callbacks=[lr_reducer, early_stopper, csv_logger, checkpoint]
                        )

# save model
model.save(path.join(f_model, '%s.h5' % model_name))
print('Model saved')
